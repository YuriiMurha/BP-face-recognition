{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Import libraries and define paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define the base directory for datasets\n",
        "BASE_DATASET_PATH = \"data/datasets\"\n",
        "\n",
        "# Dataset types and dataset sources\n",
        "DATASET_TYPES = [\"train\", \"test\", \"val\"]\n",
        "DATASET_SOURCES = [\"webcam\", \"seccam\"]\n",
        "\n",
        "# Function to generate dataset paths dynamically\n",
        "def get_dataset_paths(dataset_source, dataset_type):\n",
        "    base_path = os.path.join(BASE_DATASET_PATH, dataset_source, dataset_type)\n",
        "    return {\n",
        "        \"images\": os.path.join(base_path, \"images\", \"*.jpg\"),\n",
        "        \"labels\": os.path.join(base_path, \"labels\", \"*.json\")\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Define image and label loading function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    \"\"\"\n",
        "    Reads and decodes an image from a given file path.\n",
        "    \n",
        "    Args:\n",
        "        image_path (tf.Tensor): The file path of the image.\n",
        "    \n",
        "    Returns:\n",
        "        tf.Tensor: A decoded and normalized image tensor.\n",
        "    \"\"\"\n",
        "    image = tf.io.read_file(image_path)  # Read the image file\n",
        "    image = tf.image.decode_jpeg(image, channels=3)  # Decode the image (ensure 3 color channels)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize pixel values to [0,1]\n",
        "    return image\n",
        "\n",
        "def load_labels(label_path):\n",
        "    try:\n",
        "        with open(label_path.numpy(), 'r', encoding=\"utf-8\") as f:\n",
        "            label = json.load(f)\n",
        "        return [label['class']], label['bbox']\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {label_path}\")\n",
        "        return [], []\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error decoding JSON from file {label_path}\")\n",
        "        return [], []\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading label file {label_path}: {e}\")\n",
        "        return [], []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Create dataset pielines for labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_datasets(dataset_source):\n",
        "    datasets = {}\n",
        "\n",
        "    for dataset_type in DATASET_TYPES:\n",
        "        paths = get_dataset_paths(dataset_source, dataset_type)\n",
        "\n",
        "        # Load image dataset\n",
        "        images = tf.data.Dataset.list_files(paths[\"images\"], shuffle=False)\n",
        "        images = images.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Load label dataset\n",
        "        labels = tf.data.Dataset.list_files(paths[\"labels\"], shuffle=False)\n",
        "        labels = labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]),\n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Zip images and labels\n",
        "        dataset = tf.data.Dataset.zip((images, labels))\n",
        "        dataset = dataset.shuffle(5000 if dataset_type == \"train\" else 1000).batch(8).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        datasets[dataset_type] = dataset\n",
        "\n",
        "    return datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Combine image and label datasets to prepare the final datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "InvalidArgumentError",
          "evalue": "Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: datasets/augmented\\\\webcam\\\\train\\\\images\\\\*.jpg'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create datasets for specified sources\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m webcam_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwebcam\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Access datasets like:\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train \u001b[38;5;241m=\u001b[39m webcam_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "Cell \u001b[1;32mIn[3], line 8\u001b[0m, in \u001b[0;36mcreate_datasets\u001b[1;34m(dataset_source)\u001b[0m\n\u001b[0;32m      5\u001b[0m paths \u001b[38;5;241m=\u001b[39m get_dataset_paths(dataset_source, dataset_type)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load image dataset\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mmap(load_image, num_parallel_calls\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mAUTOTUNE)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load label dataset\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\yuram\\anaconda3\\envs\\tf39\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:1384\u001b[0m, in \u001b[0;36mDatasetV2.list_files\u001b[1;34m(file_pattern, shuffle, seed, name)\u001b[0m\n\u001b[0;32m   1377\u001b[0m condition \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mgreater(array_ops\u001b[38;5;241m.\u001b[39mshape(matching_files)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   1378\u001b[0m                              name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatch_not_empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1380\u001b[0m message \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m   1381\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo files matched pattern: \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1382\u001b[0m     string_ops\u001b[38;5;241m.\u001b[39mreduce_join(file_pattern, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1384\u001b[0m assert_not_empty \u001b[38;5;241m=\u001b[39m \u001b[43mcontrol_flow_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAssert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43massert_not_empty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcontrol_dependencies([assert_not_empty]):\n\u001b[0;32m   1387\u001b[0m   matching_files \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39midentity(matching_files)\n",
            "File \u001b[1;32mc:\\Users\\yuram\\anaconda3\\envs\\tf39\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\yuram\\anaconda3\\envs\\tf39\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py:156\u001b[0m, in \u001b[0;36mAssert\u001b[1;34m(condition, data, summarize, name)\u001b[0m\n\u001b[0;32m    154\u001b[0m     xs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_n_to_tensor(data)\n\u001b[0;32m    155\u001b[0m     data_str \u001b[38;5;241m=\u001b[39m [_summarize_eager(x, summarize) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m xs]\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError(\n\u001b[0;32m    157\u001b[0m         node_def\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    158\u001b[0m         op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    159\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to be true. Summarized data: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m    160\u001b[0m         (condition, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data_str)))\n\u001b[0;32m    161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssert\u001b[39m\u001b[38;5;124m\"\u001b[39m, [condition, data]) \u001b[38;5;28;01mas\u001b[39;00m name:\n",
            "\u001b[1;31mInvalidArgumentError\u001b[0m: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'No files matched pattern: datasets/augmented\\\\webcam\\\\train\\\\images\\\\*.jpg'"
          ]
        }
      ],
      "source": [
        "# Create datasets for specified sources\n",
        "webcam_datasets = create_datasets(\"webcam\")\n",
        "\n",
        "# Access datasets like:\n",
        "train = webcam_datasets[\"train\"]\n",
        "test = webcam_datasets[\"test\"]\n",
        "val = webcam_datasets[\"val\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Build Deep Learning using the Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Import Layers and Base Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, GlobalMaxPooling2D\n",
        "from tensorflow.keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Download VGG16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "vgg = VGG16(include_top=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "vgg.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Build instance of Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model(): \n",
        "    input_layer = Input(shape=(120,120,3))\n",
        "    \n",
        "    vgg = VGG16(include_top=False)(input_layer)\n",
        "\n",
        "    # Classification Model  \n",
        "    f1 = GlobalMaxPooling2D()(vgg)\n",
        "    class1 = Dense(2048, activation='relu')(f1)\n",
        "    class2 = Dense(1, activation='sigmoid')(class1)\n",
        "    \n",
        "    # Bounding box model\n",
        "    f2 = GlobalMaxPooling2D()(vgg)\n",
        "    regress1 = Dense(2048, activation='relu')(f2)\n",
        "    regress2 = Dense(4, activation='sigmoid')(regress1)\n",
        "    \n",
        "    facetracker = Model(inputs=input_layer, outputs=[class2, regress2])\n",
        "    return facetracker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 Test out Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "facetracker = build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 120, 120, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " vgg16 (Functional)             (None, None, None,   14714688    ['input_2[0][0]']                \n",
            "                                512)                                                              \n",
            "                                                                                                  \n",
            " global_max_pooling2d (GlobalMa  (None, 512)         0           ['vgg16[0][0]']                  \n",
            " xPooling2D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling2d_1 (Global  (None, 512)         0           ['vgg16[0][0]']                  \n",
            " MaxPooling2D)                                                                                    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 2048)         1050624     ['global_max_pooling2d[0][0]']   \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 2048)         1050624     ['global_max_pooling2d_1[0][0]'] \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1)            2049        ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 4)            8196        ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16,826,181\n",
            "Trainable params: 16,826,181\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "facetracker.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n",
            "\u001b[1;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mas_numpy_iterator()\u001b[38;5;241m.\u001b[39mnext()\n",
            "\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ],
      "source": [
        "X, y = train.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
            "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n",
            "\u001b[1;32m----> 1\u001b[0m \u001b[43mX\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
            "\n",
            "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes, coords = facetracker.predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes, coords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Define Losses and Optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Define Optimizer and LR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batches_per_epoch = len(train)\n",
        "lr_decay = (1./0.75 -1)/batches_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0001, decay=lr_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.2 Create Localization Loss and Classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def localization_loss(y_true, yhat):            \n",
        "    delta_coord = tf.reduce_sum(tf.square(y_true[:,:2] - yhat[:,:2]))\n",
        "                  \n",
        "    h_true = y_true[:,3] - y_true[:,1] \n",
        "    w_true = y_true[:,2] - y_true[:,0] \n",
        "\n",
        "    h_pred = yhat[:,3] - yhat[:,1] \n",
        "    w_pred = yhat[:,2] - yhat[:,0] \n",
        "    \n",
        "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true-h_pred))\n",
        "    \n",
        "    return delta_coord + delta_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classloss = tf.keras.losses.BinaryCrossentropy()\n",
        "regressloss = localization_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.3 Test out Loss Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "localization_loss(y[1], coords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classloss(y[0], classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "regressloss(y[1], coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Train Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Create Custom Model Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FaceTracker(Model): \n",
        "    def __init__(self, eyetracker,  **kwargs): \n",
        "        super().__init__(**kwargs)\n",
        "        self.model = eyetracker\n",
        "\n",
        "    def compile(self, opt, classloss, localizationloss, **kwargs):\n",
        "        super().compile(**kwargs)\n",
        "        self.closs = classloss\n",
        "        self.lloss = localizationloss\n",
        "        self.opt = opt\n",
        "    \n",
        "    def train_step(self, batch, **kwargs): \n",
        "        \n",
        "        X, y = batch\n",
        "        \n",
        "        with tf.GradientTape() as tape: \n",
        "            classes, coords = self.model(X, training=True)\n",
        "            \n",
        "            batch_classloss = self.closs(y[0], classes)\n",
        "            batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
        "            \n",
        "            total_loss = batch_localizationloss+0.5*batch_classloss\n",
        "            \n",
        "            grad = tape.gradient(total_loss, self.model.trainable_variables)\n",
        "        \n",
        "        opt.apply_gradients(zip(grad, self.model.trainable_variables))\n",
        "        \n",
        "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
        "    \n",
        "    def test_step(self, batch, **kwargs): \n",
        "        X, y = batch\n",
        "        \n",
        "        classes, coords = self.model(X, training=False)\n",
        "        \n",
        "        batch_classloss = self.closs(y[0], classes)\n",
        "        batch_localizationloss = self.lloss(tf.cast(y[1], tf.float32), coords)\n",
        "        total_loss = batch_localizationloss+0.5*batch_classloss\n",
        "        \n",
        "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regress_loss\":batch_localizationloss}\n",
        "        \n",
        "    def call(self, X, **kwargs): \n",
        "        return self.model(X, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = FaceTracker(facetracker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.compile(opt, classloss, regressloss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2 Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logdir='logs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist = model.fit(train, epochs=10, validation_data=val, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3 Plot Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
        "\n",
        "ax[0].plot(hist.history['total_loss'], color='teal', label='loss')\n",
        "ax[0].plot(hist.history['val_total_loss'], color='orange', label='val loss')\n",
        "ax[0].title.set_text('Loss')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(hist.history['class_loss'], color='teal', label='class loss')\n",
        "ax[1].plot(hist.history['val_class_loss'], color='orange', label='val class loss')\n",
        "ax[1].title.set_text('Classification Loss')\n",
        "ax[1].legend()\n",
        "\n",
        "ax[2].plot(hist.history['regress_loss'], color='teal', label='regress loss')\n",
        "ax[2].plot(hist.history['val_regress_loss'], color='orange', label='val regress loss')\n",
        "ax[2].title.set_text('Regression Loss')\n",
        "ax[2].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 11. Make Predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.1 Make Predictions on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_data = test.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_sample = test_data.next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yhat = facetracker.predict(test_sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
        "for idx in range(4): \n",
        "    sample_image = test_sample[0][idx]\n",
        "    sample_coords = yhat[1][idx]\n",
        "    \n",
        "    if yhat[0][idx] > 0.9:\n",
        "        cv2.rectangle(sample_image, \n",
        "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
        "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
        "                            (255,0,0), 2)\n",
        "    \n",
        "    ax[idx].imshow(sample_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2 Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "facetracker.save('custom_model.h5')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
